{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiczPXhqA35T"
   },
   "source": [
    "# CMSE 381 Final Project Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6HR6z_BA35W"
   },
   "source":[]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQ-ZENBIA35W"
   },
   "source": [
    "#### CMSE 381 Final Project\n",
    "### &#9989; Group members: Colby Degan, Jose Villegas\n",
    "### &#9989; Section_002\n",
    "#### &#9989; 4/25/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fBUG7ctA35X"
   },
   "source": [
    "# Concrete Crack Detection using SVCs and CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9RGCASwA35X"
   },
   "source": [
    "## Background and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1b4LSXwA35X"
   },
   "source": [
    " When it comes to classification problems in Data Science and Machine Learning, the utility of images can often be very useful to do so. For this particular project, a large directory of images was provided, within the directory there were two sub-directories split into categories of Cracked, and Non-cracked, then within those directories there were nested directories that consisted of the type, or structure of concrete, which were of the following: pavement, deck, and wall. Our overall goal was to consider two machine learning processes/algorithms that we learned in class, and apply those models to these different kinds of images to make a model that can optimally classify whether or not images are cracked. After these models are created, these models will be optimized utilizing some form of cross validation to make improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxsOyMX_A35Y"
   },
   "source": [
    "## Methodology\n",
    "There were two approaches that we utilized to create our models, the first of which was an Support Vector Classifier that was run on each of the three different structures, then the optimization approach was done using the GridSearchCV class from Sklearn. the other approach was done thorough A Convolutional neural network. Which is a neural network that excels in image classification, and attempts to mimic similar classification approaches that humans make. The optimization to the CNN will be to utilize gridsearch or apply dropout regularization to optimize the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2VxM5yXA35Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwryzfCgA35Z",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Data\n",
    "__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rR0xioFA35Z"
   },
   "source": [
    "We are compressing each 256x256 image into 64x64, and are using each pixel measurement of that compressed image as a variable, giving us 4096 pixel measurements. We are also using the type of structure as a variable, as it wouldn't be logical to compare a picture of cracked pavement to a picture of an un-cracked wall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJiPRlnGA35a"
   },
   "outputs": [],
   "source": [
    "\n",
    "folder_paths = [\"archive/Decks/Cracked\", \"archive/Decks/Non-cracked\", \"archive/Pavements/Cracked\", \"archive/Pavements/Non-cracked\", \"archive/Walls/Cracked\", \"archive/Walls/Non-cracked\"]\n",
    "image_data = []\n",
    "cracked = []\n",
    "structure = []\n",
    "\n",
    "# For every folder that contains images\n",
    "for folder_path in folder_paths:\n",
    "\n",
    "    # Get list of image files\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "\n",
    "    # Initialize list to store image data\n",
    "    for i in range(len(image_files)):\n",
    "        img_path = os.path.join(folder_path, image_files[i])\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img.thumbnail((16, 16), Image.Resampling.LANCZOS)\n",
    "        img_array = np.array(img).flatten()\n",
    "        image_data.append(img_array)\n",
    "        if folder_path == \"archive/Decks/Cracked\" or folder_path == \"archive/Decks/Non-cracked\":\n",
    "            structure.append(\"Deck\")\n",
    "        elif folder_path == \"archive/Pavements/Cracked\" or folder_path == \"archive/Pavements/Non-cracked\":\n",
    "            structure.append(\"Pavement\")\n",
    "        else:\n",
    "            structure.append(\"Wall\")\n",
    "        if folder_path == \"archive/Decks/Cracked\" or folder_path == \"archive/Pavements/Cracked\" or folder_path == \"archive/Walls/Cracked\":\n",
    "            cracked.append(1)\n",
    "        else:\n",
    "            cracked.append(0)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(image_data)\n",
    "df[\"Structure\"] = structure\n",
    "df[\"Cracked\"] = cracked\n",
    "df.to_csv(\"concrete_dataset_16.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteDataset(Dataset):\n",
    "    \"\"\" Object set to take in images from the provided dataset\n",
    "\n",
    "        The reason for this Class is to be able to utilize DataLoaders for the CNN model, \n",
    "        two different approaches were used to read in the data depending on the model being used.\n",
    "    \"\"\"\n",
    "    def __init__(self, folder_paths, transform=None):\n",
    "        \"\"\" Initialization function consisting of:\n",
    "\n",
    "        - image data: \n",
    "        - Cracked: list for all cracked images (1) or non-cracked (0)\n",
    "        - structure: list for structure types in concrete (wall, pavement, decks)\n",
    "        \"\"\"\n",
    "        self.image_data = []\n",
    "        self.cracked = []\n",
    "        self.structure = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        # For each folder (cracked or non-cracked)\n",
    "        for folder_path in folder_paths:\n",
    "            # Get list of image files\n",
    "            image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "\n",
    "            for filename in image_files:\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                img = Image.open(img_path).convert('L')  # Grayscale image\n",
    "                img.thumbnail((16, 16), Image.Resampling.LANCZOS)\n",
    "                img_array = np.array(img)  # Keep as 2D array\n",
    "\n",
    "                # Pad to exactly 64x64 if needed\n",
    "                if img_array.shape != (16, 16):\n",
    "                    padded_img = np.zeros((16, 16), dtype=np.uint8)\n",
    "                    padded_img[:img_array.shape[0], :img_array.shape[1]] = img_array\n",
    "                    img_array = padded_img\n",
    "\n",
    "                # Append image data\n",
    "                self.image_data.append(img_array)\n",
    "                \n",
    "                # Append structure type\n",
    "                self.structure.append(folder_path.split('/')[0])  # Extract structure (Deck, Pavement, Wall)\n",
    "                \n",
    "                # Append cracked/non-cracked label\n",
    "                if 'Cracked' in folder_path:\n",
    "                    self.cracked.append(1)\n",
    "                else:\n",
    "                    self.cracked.append(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Helpful length function to get size of Dataset Easily \"\"\"\n",
    "        return len(self.image_data)\n",
    "                    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" This function allows for easy access of the data's  image at a particular index\n",
    "            and the images' respective label, 0 representing Cracked, and 1 representing Non-Cracked\n",
    "        \"\"\"\n",
    "        image = self.image_data[idx]\n",
    "        label = self.cracked[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = [\n",
    "    \"Decks/Cracked\", \"Decks/Non-cracked\", \n",
    "    \"Pavements/Cracked\", \"Pavements/Non-cracked\", \n",
    "    \"Walls/Cracked\", \"Walls/Non-cracked\"\n",
    "]\n",
    "image_data = []\n",
    "cracked = []\n",
    "structure = []\n",
    "\n",
    "# Creating a transformation pipeline which normalizes images to get into correct range\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [0, 1] range\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Concrete_Images = ConcreteDataset(folder_paths, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(Concrete_Images))  # 80% for training\n",
    "test_size = len(Concrete_Images) - train_size\n",
    "train_dataset, test_dataset = random_split(Concrete_Images, [train_size, test_size])\n",
    "\n",
    "# Utilizing dataloaders in order to process batches of images instead of attempting to do it all at once\n",
    "# this makes things less computationally expensive\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "ng2P3tl7A35a",
    "outputId": "9dda09d9-c107-4362-fae5-16d1a1b2bdd1"
   },
   "outputs": [],
   "source": [
    "concrete_dataset = pd.read_csv('concrete_dataset_16.csv')\n",
    "concrete_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "F9NogGfzA35a",
    "outputId": "01a1b3b5-e0cf-44be-fc28-36befd8e722f"
   },
   "outputs": [],
   "source": [
    "concrete_dataset = pd.get_dummies(concrete_dataset, drop_first = True)\n",
    "concrete_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUGqeKqqA35a"
   },
   "source": [
    "### Models for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5foDo2rA35b"
   },
   "source": [
    "#### SVC with GridSearchCV\n",
    "\n",
    "We chose this because we thought it would be useful for image classification. By using GridSearchCV we are able to check what the best possible parameters are for the model. We will evaluate this model by checking its training and testing errors to see how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GSNTfCV1A35b",
    "outputId": "2406807f-182b-4dfa-f092-df163c9b1467"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = concrete_dataset.drop(columns = [\"Cracked\"])\n",
    "y = concrete_dataset.Cracked\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "smote = SMOTE(random_state = 42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "svc = SVC(class_weight='balanced', random_state=42)\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear'], 'gamma': ['scale', 'auto', 0.1]}\n",
    "\n",
    "grid_search = GridSearchCV(svc, param_grid, cv = 5, scoring = 'accuracy', n_jobs = -1)\n",
    "grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "best_svc = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_EmP6g1A35b"
   },
   "source": [
    "#### CNN with Nested Loop Hyperparameter Search\n",
    "\n",
    "We chose this because we thought it would be useful to identify smaller changes in the model, which is useful for noticing small cracks. We were unable to properly use GridSearchCV for the CNN and Pytoarch and Sklearn are incompatible. To get around this we wrote a custom nested loop hyperparameter search to determine the best hyperparameters for the CNN. We will evaluate the CNN using accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tmo9UqILA35b"
   },
   "outputs": [],
   "source": [
    "CNN_model = nn.Sequential(\n",
    "    # This is the Convolutional Layer, with the single input being the image itself, however the image itself\n",
    "    # Is a 4 dimensional Tensor object, as this is how Pytorch CNNs are implemented\n",
    "    # The kernel size is representative of the matrix that actually does the convolving over the input\n",
    "    nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "    nn.ReLU(), # Using ReLU \n",
    "    nn.MaxPool2d(kernel_size=2, stride=2), # Pooling layer to find the maximum value to represent the 2x2 pooling sample of our image\n",
    "\n",
    "    # A hidden Convolutional layer reading the output of the previous (size 16) and using a similar kernel size to convolve over current image\n",
    "    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "    nn.ReLU(), # ReLU once again as activation\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2), # Pooling layer used to similar find max of 2x2 pooling matrix\n",
    "\n",
    "    nn.Flatten(),  # Using flatten to turn 32, 16, 16 into a single value\n",
    "    nn.Linear(32 * 4 * 4, 64),\n",
    "    nn.ReLU(), # ReLU!\n",
    "    nn.Linear(64, 1)  # One output for binary classification\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch device is able to use GPU to run model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CNN_model = CNN_model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(CNN_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# Each epoch iteration\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Training CNN\n",
    "    CNN_model.train()\n",
    "\n",
    "    # Loss variable to track total loss\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Now iterating through dataloader to limit image processing to make more managable\n",
    "    for images, labels in train_loader: \n",
    "\n",
    "        # Moving the images and labels to the same device as model for computations\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "        outputs = CNN_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "all_predicts = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predictions = torch.sigmoid(outputs) > 0.5\n",
    "        \n",
    "        all_predict.extend(predictions.cpu().numpy())\n",
    "        all_labelsextend(labels.cpu().numpy())\n",
    "        \n",
    "accuracy = sum(np.array(all_predicts) == np.array(all_labels)) / len(all_labels)\n",
    "print(f\"Accuracy: {100 * accuracy:.2f}%\")\n",
    "\n",
    "print(classification_report(all_labels, all_predicts, target_named=[\"Uncracked\", \"Cracked\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Manual Grid Search optimization\n",
    "\n",
    "The Benefit to using A grid seach validation method would be to find the most optimal selection of parameters, those that were considered were, optimizer, batch size, and learning rate. The reason this approach was used is because of one main issue, Sklearns gridsearch doesn't have a direct application with pytorch, and because of this, the product function is used from itertools to create matrix of different learning rates, batches, and optimizers to find an optimal model, this will run several CNNs over the course of what is likely a long time. The results were evaluated by getting the ratio of correctly classified images over the total number of images looked at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.001]\n",
    "batches = [16, 32, 64]\n",
    "\n",
    "grid_iter = list(product(learning_rates, batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for lr, curr_batch, opt in grid_iter:\n",
    "    print(f\"\\n Current learning rate: {lr}, batch size: {curr_batch}, Optimizer: {opt}\")\n",
    "\n",
    "    \n",
    "    # The reason for making different loaders for every instance is due to the different batch sizes to find most optimal hyperparameter\n",
    "    curr_loader = DataLoader(Concrete_Images, batch_size = curr_batch, shuffle = True)\n",
    "\n",
    "\n",
    "    # Creating a model for the multiple iterations\n",
    "    curr_model = nn.Sequential(\n",
    "        nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "\n",
    "        nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32 * 16 * 16, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 1)\n",
    "    ).to(device)\n",
    "\n",
    "    # Need to have specific conditions to use different optimizers when neccessary \n",
    "\n",
    "    optimizer = optim.Adam(curr_model.parameters(), lr=lr)\n",
    "\n",
    "    # Need our binary classifier using Binary classification log loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Now we need to actually train our model(s) will be done with set amount of epochs (5)\n",
    "    for epoch in range(5):\n",
    "\n",
    "        # Call train function on the \n",
    "        curr_model.train()\n",
    "        constant_loss = 0.0\n",
    "\n",
    "        # Now iterate through the images and their respective layers through our DataLoader for Concrete data\n",
    "        for images, labels in curr_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            # Reset gradients so each model doesn't have incorrect gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Store outputs as model run on images\n",
    "            outputs = curr_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Add to constant loss to get overall loss by end of model run\n",
    "            constant_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {constant_loss:.4f}\")\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    curr_model.eval()\n",
    "\n",
    "    # Code to get accuracy of particular model\n",
    "    with torch.no_grad():\n",
    "        for images, labels in curr_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "    \n",
    "            outputs = curr_model(images)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'batch_size': curr_batch,\n",
    "        'optimizer': opt,\n",
    "        'accuracy': accuracy\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d05nhNuPA35b"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeefecE1A35c"
   },
   "source": [
    "_(What did you find when you carried out your methods? Some of your code related to\n",
    "presenting results/figures/data may be replicated from the methods section or may only be present in\n",
    "this section. All of the plots that you plan on using for your presentation should be present in this\n",
    "section)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHJ8Hq5aA35c"
   },
   "source": [
    "### SVC results\n",
    "_(What are you trying to do here?)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "86aEgOtGA35c",
    "outputId": "4f5d8aa3-6447-4068-e83f-21414943dec5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = best_svc.predict(X_test_scaled)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGOViASJA35c"
   },
   "source": [
    "_(How do you interpret what you see?)_\n",
    "\n",
    "_(What are you doing next?)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKJLLwAWA35c"
   },
   "outputs": [],
   "source": [
    "# how did you do it (etc. etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hbG2zMVA35c"
   },
   "source": [
    "### CNN results\n",
    "\n",
    "The initial model had a learning rate of 0.001, using the Adam optimizer, and a batch size of 32\n",
    "\n",
    "This model didn’t perform that poorly, but a question of which hyperparameters were best was yet to be answered. Because of this the optimization process would then turn into a gridsearch process where multiple arrays of hyperparameters would be used to run multiple different models, because of the large amount of Neural Networks this would create, some things had to be toned down, like number of epochs for example. Along with this in order to reduce runtime, images were scaled down to 16x16 pixels. This may have led to some incorrect classifications but also resulted in quicker results. Unfortunately, the typical parameters used to assess neural networks, that being f1-score, precision, and recall were not programmed in before running the model, so going based off accuracy of classification is the only parameter being discussed.\n",
    "\n",
    "\n",
    "The initial model is on the left, which had an okay accuracy of 86%, with 10 epochs to run through. The optimized model(s) however have varying results, The model with the configuration Adam, learning rate 0.01, and batch size 64 had the greatest overall loss, whereas the SGD, 0.01, 64, configuration maintained consistency, but was slower than the Adam variation. A model that does not do very well is the model with optimizer SGD, learning rate 0.01, and batch size 16, as the loss drop is not a significant, which indicates that the model is not learning as well, this may be due to the small batch sizes not doing well with the faster learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlXb_iBuA35c"
   },
   "outputs": [],
   "source": [
    "results_16 = {\n",
    "    (\"SGD\", 0.01, 16): [1445.4743, 1441.6831, 1438.9502, 1437.0728, 1433.2455],\n",
    "    (\"Adam\", 0.01, 16): [1475.2849, 1460.9638, 1460.2559, 1459.9407, 1460.0902],\n",
    "    (\"SGD\", 0.001, 16): [1492.5712, 1444.7881, 1443.2383, 1442.3714, 1441.5324],\n",
    "    (\"Adam\", 0.001, 16): [1389.5200, 1279.5868, 1181.0295, 1164.8745, 1148.8653]\n",
    "}\n",
    "\n",
    "results_32 = {\n",
    "    (\"SGD\", 0.01, 32): [730.2184, 720.4164, 719.2340, 719.3720, 718.7629],\n",
    "    (\"Adam\", 0.01, 32): [713.6464, 670.1269, 645.0783, 616.9844, 592.5035]\n",
    "}\n",
    "\n",
    "results_64 = {\n",
    "    (\"SGD\", 0.01, 64): [366.5066, 360.8374, 359.9431, 359.1196, 358.6196],\n",
    "    (\"Adam\", 0.01, 64): [364.7134, 356.2679, 341.4328, 324.2015, 315.0432],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (opt, lr, bs), losses in results_16.items():\n",
    "    label = f\"{opt}, lr={lr}, bs={bs}\"\n",
    "    plt.plot(range(1, len(losses)+1), losses, label=label)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch for Different Hyperparameters')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (opt, lr, bs), losses in results_32.items():\n",
    "    label = f\"{opt}, lr={lr}, bs={bs}\"\n",
    "    plt.plot(range(1, len(losses)+1), losses, label=label)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Epoch for different Hyperparameters\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (opt, lr, bs), losses in results_64.items():\n",
    "    label = f\"{opt}, lr={lr}, bs={bs}\"\n",
    "    plt.plot(range(1, len(losses)+1), losses, label=label)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Epoch for different Hyperparameters\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfqqnObIA35c"
   },
   "source": [
    "## Discussion and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsEYB7isA35h"
   },
   "source": [
    "### Discussion on the CNN Results\n",
    "The optimized model unfortunately had to be cut short, as 48 hours in, we had around 9 CNNs that had run with a selection of different hyperparameters. Some models had a steadily decreasing loss which indicates that the models were learning at an okay rate. Due to the model not being able to test all parameters, we don’t have a full set of results to analyze, and we don’t have all of our assessment parameters to work with either\n",
    "\n",
    "In the future, this optimization process will have all assessment feature (precision, recall, f1-score and accuracy) implemented in order to analyze the model completely In addition to this, the model would ideally be run on the images in a larger, more interpretable resolution. Not to mention, the selection of hyperparameters will all be evaluated, even if it takes multiple days to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FV7AxGoA35h"
   },
   "source": [
    "## Author contribution\n",
    "\n",
    "Colby Degan - Responsible for making the SVC\n",
    "\n",
    "Jose Villegas - Responsible for making the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzWeu0nMA35h"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsoyKxQlA35h"
   },
   "source": [
    "SciKit Learn. (2019). sklearn.model_selection.GridSearchCV — scikit-learn 0.22 Documentation. Scikit-Learn.org. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "Pytorch. PyTorch documentation.  https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "Geeksforgeeks. (2025). Building a Convolutional Neural Network using PyTorch. https://www.geeksforgeeks.org/building-a-convolutional-neural-network-using-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.12 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
