{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiczPXhqA35T"
   },
   "source": [
    "# CMSE 381 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQ-ZENBIA35W"
   },
   "source": [
    "#### CMSE 381 Final Project\n",
    "### &#9989; Group members: Colby Degan, Jose Villegas\n",
    "### &#9989; Section_002\n",
    "#### &#9989; 4/25/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fBUG7ctA35X"
   },
   "source": [
    "# Concrete Crack Detection using SVCs and CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9RGCASwA35X"
   },
   "source": [
    "## Background and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1b4LSXwA35X"
   },
   "source": [
    " When it comes to classification problems in Data Science and Machine Learning, the utility of images can often be very useful to do so. For this particular project, a large directory of images was provided, within the directory there were two sub-directories split into categories of Cracked, and Non-cracked, then within those directories there were nested directories that consisted of the type, or structure of concrete, which were of the following: pavement, deck, and wall. Our overall goal was to consider two machine learning processes/algorithms that we learned in class, and apply those models to these different kinds of images to make a model that can optimally classify whether or not images are cracked. After these models are created, these models will be optimized utilizing some form of cross validation to make improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxsOyMX_A35Y"
   },
   "source": [
    "## Methodology\n",
    "There were two approaches that we utilized to create our models, the first of which was an Support Vector Classifier that was run on each of the three different structures, then the optimization approach was done using the GridSearchCV class from Sklearn. the other approach was done thorough A Convolutional neural network. Which is a neural network that excels in image classification, and attempts to mimic similar classification approaches that humans make. The optimization to the CNN will be to utilize gridsearch or apply dropout regularization to optimize the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n2VxM5yXA35Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwryzfCgA35Z",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rR0xioFA35Z"
   },
   "source": [
    "We are compressing each 256x256 image into 16x16, and are using each pixel measurement of that compressed image as a variable, giving us 256 pixel measurements. We are also using the type of structure as a variable, as it wouldn't be logical to compare a picture of cracked pavement to a picture of an un-cracked wall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJiPRlnGA35a"
   },
   "outputs": [],
   "source": [
    "\n",
    "folder_paths = [\"archive/Decks/Cracked\", \"archive/Decks/Non-cracked\", \"archive/Pavements/Cracked\", \"archive/Pavements/Non-cracked\", \"archive/Walls/Cracked\", \"archive/Walls/Non-cracked\"]\n",
    "image_data = []\n",
    "cracked = []\n",
    "structure = []\n",
    "\n",
    "# For every folder that contains images\n",
    "for folder_path in folder_paths:\n",
    "\n",
    "    # Get list of image files\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "\n",
    "    # Initialize list to store image data\n",
    "    for i in range(len(image_files)):\n",
    "        img_path = os.path.join(folder_path, image_files[i])\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img.thumbnail((16, 16), Image.Resampling.LANCZOS)\n",
    "        img_array = np.array(img).flatten()\n",
    "        image_data.append(img_array)\n",
    "        if folder_path == \"archive/Decks/Cracked\" or folder_path == \"archive/Decks/Non-cracked\":\n",
    "            structure.append(\"Deck\")\n",
    "        elif folder_path == \"archive/Pavements/Cracked\" or folder_path == \"archive/Pavements/Non-cracked\":\n",
    "            structure.append(\"Pavement\")\n",
    "        else:\n",
    "            structure.append(\"Wall\")\n",
    "        if folder_path == \"archive/Decks/Cracked\" or folder_path == \"archive/Pavements/Cracked\" or folder_path == \"archive/Walls/Cracked\":\n",
    "            cracked.append(1)\n",
    "        else:\n",
    "            cracked.append(0)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(image_data)\n",
    "df[\"Structure\"] = structure\n",
    "df[\"Cracked\"] = cracked\n",
    "df.to_csv(\"concrete_dataset_16.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteDataset(Dataset):\n",
    "    \"\"\" Object set to take in images from the provided dataset\n",
    "\n",
    "        The reason for this Class is to be able to utilize DataLoaders for the CNN model, \n",
    "        two different approaches were used to read in the data depending on the model being used.\n",
    "    \"\"\"\n",
    "    def __init__(self, folder_paths, transform=None):\n",
    "        \"\"\" Initialization function consisting of:\n",
    "\n",
    "        - image data: \n",
    "        - Cracked: list for all cracked images (1) or non-cracked (0)\n",
    "        - structure: list for structure types in concrete (wall, pavement, decks)\n",
    "        \"\"\"\n",
    "        self.image_data = []\n",
    "        self.cracked = []\n",
    "        self.structure = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        # For each folder (cracked or non-cracked)\n",
    "        for folder_path in folder_paths:\n",
    "            # Get list of image files\n",
    "            image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "\n",
    "            for filename in image_files:\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                img = Image.open(img_path).convert('L')  # Grayscale image\n",
    "                img.thumbnail((16, 16), Image.Resampling.LANCZOS)\n",
    "                img_array = np.array(img)  # Keep as 2D array\n",
    "\n",
    "                # Pad to exactly 64x64 if needed\n",
    "                if img_array.shape != (16, 16):\n",
    "                    padded_img = np.zeros((16, 16), dtype=np.uint8)\n",
    "                    padded_img[:img_array.shape[0], :img_array.shape[1]] = img_array\n",
    "                    img_array = padded_img\n",
    "\n",
    "                # Append image data\n",
    "                self.image_data.append(img_array)\n",
    "                \n",
    "                # Append structure type\n",
    "                self.structure.append(folder_path.split('/')[0])  # Extract structure (Deck, Pavement, Wall)\n",
    "                \n",
    "                # Append cracked/non-cracked label\n",
    "                if 'Cracked' in folder_path:\n",
    "                    self.cracked.append(1)\n",
    "                else:\n",
    "                    self.cracked.append(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Helpful length function to get size of Dataset Easily \"\"\"\n",
    "        return len(self.image_data)\n",
    "                    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" This function allows for easy access of the data's  image at a particular index\n",
    "            and the images' respective label, 0 representing Cracked, and 1 representing Non-Cracked\n",
    "        \"\"\"\n",
    "        image = self.image_data[idx]\n",
    "        label = self.cracked[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = [\n",
    "    \"Decks/Cracked\", \"Decks/Non-cracked\", \n",
    "    \"Pavements/Cracked\", \"Pavements/Non-cracked\", \n",
    "    \"Walls/Cracked\", \"Walls/Non-cracked\"\n",
    "]\n",
    "image_data = []\n",
    "cracked = []\n",
    "structure = []\n",
    "\n",
    "# Creating a transformation pipeline which normalizes images to get into correct range\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [0, 1] range\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Concrete_Images = ConcreteDataset(folder_paths, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(Concrete_Images))  # 80% for training\n",
    "test_size = len(Concrete_Images) - train_size\n",
    "train_dataset, test_dataset = random_split(Concrete_Images, [train_size, test_size])\n",
    "\n",
    "# Utilizing dataloaders in order to process batches of images instead of attempting to do it all at once\n",
    "# this makes things less computationally expensive\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "ng2P3tl7A35a",
    "outputId": "9dda09d9-c107-4362-fae5-16d1a1b2bdd1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "concrete_dataset"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-2494552d-d650-4a54-84de-56fc64165ef7\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>Structure</th>\n",
       "      <th>Cracked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>179</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>179</td>\n",
       "      <td>177</td>\n",
       "      <td>180</td>\n",
       "      <td>181</td>\n",
       "      <td>179</td>\n",
       "      <td>175</td>\n",
       "      <td>178</td>\n",
       "      <td>...</td>\n",
       "      <td>177</td>\n",
       "      <td>181</td>\n",
       "      <td>185</td>\n",
       "      <td>183</td>\n",
       "      <td>186</td>\n",
       "      <td>191</td>\n",
       "      <td>183</td>\n",
       "      <td>181</td>\n",
       "      <td>Deck</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>176</td>\n",
       "      <td>178</td>\n",
       "      <td>180</td>\n",
       "      <td>177</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>181</td>\n",
       "      <td>186</td>\n",
       "      <td>...</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>186</td>\n",
       "      <td>177</td>\n",
       "      <td>182</td>\n",
       "      <td>180</td>\n",
       "      <td>Deck</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>182</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>196</td>\n",
       "      <td>193</td>\n",
       "      <td>190</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>...</td>\n",
       "      <td>189</td>\n",
       "      <td>187</td>\n",
       "      <td>189</td>\n",
       "      <td>191</td>\n",
       "      <td>186</td>\n",
       "      <td>192</td>\n",
       "      <td>190</td>\n",
       "      <td>189</td>\n",
       "      <td>Deck</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>199</td>\n",
       "      <td>194</td>\n",
       "      <td>176</td>\n",
       "      <td>175</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>179</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>181</td>\n",
       "      <td>177</td>\n",
       "      <td>182</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>181</td>\n",
       "      <td>179</td>\n",
       "      <td>177</td>\n",
       "      <td>Deck</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>182</td>\n",
       "      <td>177</td>\n",
       "      <td>179</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>181</td>\n",
       "      <td>...</td>\n",
       "      <td>180</td>\n",
       "      <td>175</td>\n",
       "      <td>174</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>178</td>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>Deck</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 258 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2494552d-d650-4a54-84de-56fc64165ef7')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-2494552d-d650-4a54-84de-56fc64165ef7 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-2494552d-d650-4a54-84de-56fc64165ef7');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-f8d51435-a495-4ee8-a3a9-95cd8a5d86e5\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f8d51435-a495-4ee8-a3a9-95cd8a5d86e5')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-f8d51435-a495-4ee8-a3a9-95cd8a5d86e5 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  248  249  250  251  \\\n",
       "0  179  172  172  179  177  180  181  179  175  178  ...  177  181  185  183   \n",
       "1  181  182  176  178  180  177  180  179  181  186  ...  185  185  180  179   \n",
       "2  182  188  187  196  193  190  185  185  188  188  ...  189  187  189  191   \n",
       "3  199  194  176  175  186  186  184  184  179  170  ...  181  177  182  178   \n",
       "4  176  180  179  182  177  179  177  178  178  181  ...  180  175  174  180   \n",
       "\n",
       "   252  253  254  255  Structure  Cracked  \n",
       "0  186  191  183  181       Deck        1  \n",
       "1  186  177  182  180       Deck        1  \n",
       "2  186  192  190  189       Deck        1  \n",
       "3  178  181  179  177       Deck        1  \n",
       "4  179  178  181  182       Deck        1  \n",
       "\n",
       "[5 rows x 258 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_dataset = pd.read_csv('concrete_dataset_16.csv')\n",
    "concrete_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "F9NogGfzA35a",
    "outputId": "01a1b3b5-e0cf-44be-fc28-36befd8e722f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "concrete_dataset"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-572cdce0-d8b5-4cca-8f43-fa26211f5f3b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>Cracked</th>\n",
       "      <th>Structure_Pavement</th>\n",
       "      <th>Structure_Wall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>179</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>179</td>\n",
       "      <td>177</td>\n",
       "      <td>180</td>\n",
       "      <td>181</td>\n",
       "      <td>179</td>\n",
       "      <td>175</td>\n",
       "      <td>178</td>\n",
       "      <td>...</td>\n",
       "      <td>181</td>\n",
       "      <td>185</td>\n",
       "      <td>183</td>\n",
       "      <td>186</td>\n",
       "      <td>191</td>\n",
       "      <td>183</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>176</td>\n",
       "      <td>178</td>\n",
       "      <td>180</td>\n",
       "      <td>177</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>181</td>\n",
       "      <td>186</td>\n",
       "      <td>...</td>\n",
       "      <td>185</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>186</td>\n",
       "      <td>177</td>\n",
       "      <td>182</td>\n",
       "      <td>180</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>182</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>196</td>\n",
       "      <td>193</td>\n",
       "      <td>190</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>...</td>\n",
       "      <td>187</td>\n",
       "      <td>189</td>\n",
       "      <td>191</td>\n",
       "      <td>186</td>\n",
       "      <td>192</td>\n",
       "      <td>190</td>\n",
       "      <td>189</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>199</td>\n",
       "      <td>194</td>\n",
       "      <td>176</td>\n",
       "      <td>175</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>179</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>177</td>\n",
       "      <td>182</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>181</td>\n",
       "      <td>179</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>182</td>\n",
       "      <td>177</td>\n",
       "      <td>179</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>181</td>\n",
       "      <td>...</td>\n",
       "      <td>175</td>\n",
       "      <td>174</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>178</td>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 259 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-572cdce0-d8b5-4cca-8f43-fa26211f5f3b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-572cdce0-d8b5-4cca-8f43-fa26211f5f3b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-572cdce0-d8b5-4cca-8f43-fa26211f5f3b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-b7bd8969-4aee-43fa-b4b9-3e3bcb5fe4e3\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b7bd8969-4aee-43fa-b4b9-3e3bcb5fe4e3')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-b7bd8969-4aee-43fa-b4b9-3e3bcb5fe4e3 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  249  250  251  252  \\\n",
       "0  179  172  172  179  177  180  181  179  175  178  ...  181  185  183  186   \n",
       "1  181  182  176  178  180  177  180  179  181  186  ...  185  180  179  186   \n",
       "2  182  188  187  196  193  190  185  185  188  188  ...  187  189  191  186   \n",
       "3  199  194  176  175  186  186  184  184  179  170  ...  177  182  178  178   \n",
       "4  176  180  179  182  177  179  177  178  178  181  ...  175  174  180  179   \n",
       "\n",
       "   253  254  255  Cracked  Structure_Pavement  Structure_Wall  \n",
       "0  191  183  181        1               False           False  \n",
       "1  177  182  180        1               False           False  \n",
       "2  192  190  189        1               False           False  \n",
       "3  181  179  177        1               False           False  \n",
       "4  178  181  182        1               False           False  \n",
       "\n",
       "[5 rows x 259 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_dataset = pd.get_dummies(concrete_dataset, drop_first = True)\n",
    "concrete_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUGqeKqqA35a"
   },
   "source": [
    "### Models for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5foDo2rA35b"
   },
   "source": [
    "#### SVC with GridSearchCV\n",
    "\n",
    "We chose this because we thought it would be useful for image classification. By using GridSearchCV we are able to check what the best possible parameters are for the model. We will evaluate this model by checking its training and testing errors to see how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GSNTfCV1A35b",
    "outputId": "2406807f-182b-4dfa-f092-df163c9b1467"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Obtain X and y data from dataset\n",
    "X = concrete_dataset.drop(columns = [\"Cracked\"])\n",
    "y = concrete_dataset.Cracked\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 42)\n",
    "\n",
    "# Scales the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Handles imbalances within the data\n",
    "smote = SMOTE(random_state = 42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Runs Grid Search with multiple different parameters to find the best parameters for the data\n",
    "svc = SVC(class_weight='balanced', random_state=42)\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear'], 'gamma': ['scale', 'auto', 0.1]}\n",
    "grid_search = GridSearchCV(svc, param_grid, cv = 5, scoring = 'accuracy', n_jobs = -1)\n",
    "grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Chooses the best SVC found from the best parameters\n",
    "best_svc = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_EmP6g1A35b"
   },
   "source": [
    "#### CNN with Nested Loop Hyperparameter Search\n",
    "\n",
    "We chose this because we thought it would be useful to identify smaller changes in the model, which is useful for noticing small cracks. We were unable to properly use GridSearchCV for the CNN and Pytoarch and Sklearn are incompatible. To get around this we wrote a custom nested loop hyperparameter search to determine the best hyperparameters for the CNN. We will evaluate the CNN using accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tmo9UqILA35b"
   },
   "outputs": [],
   "source": [
    "CNN_model = nn.Sequential(\n",
    "    # This is the Convolutional Layer, with the single input being the image itself, however the image itself\n",
    "    # Is a 4 dimensional Tensor object, as this is how Pytorch CNNs are implemented\n",
    "    # The kernel size is representative of the matrix that actually does the convolving over the input\n",
    "    nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "    nn.ReLU(), # Using ReLU \n",
    "    nn.MaxPool2d(kernel_size=2, stride=2), # Pooling layer to find the maximum value to represent the 2x2 pooling sample of our image\n",
    "\n",
    "    # A hidden Convolutional layer reading the output of the previous (size 16) and using a similar kernel size to convolve over current image\n",
    "    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "    nn.ReLU(), # ReLU once again as activation\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2), # Pooling layer used to similar find max of 2x2 pooling matrix\n",
    "\n",
    "    nn.Flatten(),  # Using flatten to turn 32, 16, 16 into a single value\n",
    "    nn.Linear(32 * 4 * 4, 64),\n",
    "    nn.ReLU(), # ReLU!\n",
    "    nn.Linear(64, 1)  # One output for binary classification\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch device is able to use GPU to run model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CNN_model = CNN_model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(CNN_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# Each epoch iteration\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Training CNN\n",
    "    CNN_model.train()\n",
    "\n",
    "    # Loss variable to track total loss\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Now iterating through dataloader to limit image processing to make more managable\n",
    "    for images, labels in train_loader: \n",
    "\n",
    "        # Moving the images and labels to the same device as model for computations\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "        outputs = CNN_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "all_predicts = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predictions = torch.sigmoid(outputs) > 0.5\n",
    "        \n",
    "        all_predict.extend(predictions.cpu().numpy())\n",
    "        all_labelsextend(labels.cpu().numpy())\n",
    "        \n",
    "accuracy = sum(np.array(all_predicts) == np.array(all_labels)) / len(all_labels)\n",
    "print(f\"Accuracy: {100 * accuracy:.2f}%\")\n",
    "\n",
    "print(classification_report(all_labels, all_predicts, target_named=[\"Uncracked\", \"Cracked\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Manual Grid Search optimization\n",
    "\n",
    "The Benefit to using A grid seach validation method would be to find the most optimal selection of parameters, those that were considered were, optimizer, batch size, and learning rate. The reason this approach was used is because of one main issue, Sklearns gridsearch doesn't have a direct application with pytorch, and because of this, the product function is used from itertools to create matrix of different learning rates, batches, and optimizers to find an optimal model, this will run several CNNs over the course of what is likely a long time. The results were evaluated by getting the ratio of correctly classified images over the total number of images looked at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.001]\n",
    "batches = [16, 32, 64]\n",
    "\n",
    "grid_iter = list(product(learning_rates, batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for lr, curr_batch, opt in grid_iter:\n",
    "    print(f\"\\n Current learning rate: {lr}, batch size: {curr_batch}, Optimizer: {opt}\")\n",
    "\n",
    "    \n",
    "    # The reason for making different loaders for every instance is due to the different batch sizes to find most optimal hyperparameter\n",
    "    curr_loader = DataLoader(Concrete_Images, batch_size = curr_batch, shuffle = True)\n",
    "\n",
    "\n",
    "    # Creating a model for the multiple iterations\n",
    "    curr_model = nn.Sequential(\n",
    "        nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "\n",
    "        nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32 * 16 * 16, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 1)\n",
    "    ).to(device)\n",
    "\n",
    "    # Need to have specific conditions to use different optimizers when neccessary \n",
    "\n",
    "    optimizer = optim.Adam(curr_model.parameters(), lr=lr)\n",
    "\n",
    "    # Need our binary classifier using Binary classification log loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Now we need to actually train our model(s) will be done with set amount of epochs (5)\n",
    "    for epoch in range(5):\n",
    "\n",
    "        # Call train function on the \n",
    "        curr_model.train()\n",
    "        constant_loss = 0.0\n",
    "\n",
    "        # Now iterate through the images and their respective layers through our DataLoader for Concrete data\n",
    "        for images, labels in curr_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            # Reset gradients so each model doesn't have incorrect gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Store outputs as model run on images\n",
    "            outputs = curr_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Add to constant loss to get overall loss by end of model run\n",
    "            constant_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {constant_loss:.4f}\")\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    curr_model.eval()\n",
    "\n",
    "    # Code to get accuracy of particular model\n",
    "    with torch.no_grad():\n",
    "        for images, labels in curr_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "    \n",
    "            outputs = curr_model(images)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'batch_size': curr_batch,\n",
    "        'optimizer': opt,\n",
    "        'accuracy': accuracy\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d05nhNuPA35b"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHJ8Hq5aA35c"
   },
   "source": [
    "### SVC results\n",
    "\n",
    "The most optimal SVC found by GridSearchCV obtain an accuracy of around 78.3% when using the scaled, balanced data. This isn't necessarily a bad accuracy, but we would prefer to have one higher, preferably around 90%. The precision, recall, and f1-score for no crack being found were 0.87, 0.88, and 0.87, respectively, while those for a crack being found were 0.26, 0.23, and 0.24. This is likely due to there being more images of concrete without cracks than those with cracks, as evidenced by the support values of 9522 for no cracks found and 1697 for a crack being found.\n",
    "\n",
    "We additionally generated a confusion matrix to better understand and visualize the model's performance across the classes. The confusion matrix showcases that there were 1128 false positives and 1308 false negatives. It also shows that around 88.2% of the images without cracks were identified correctly, while only 22.9% of the images with cracks were identified correctly. As mentioned earlier this is likely due to the difference in size between the two classes. Another likely cause of this are the small, complex patterns that cracks cause in the image data, which SVCs have a hard time noticing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "86aEgOtGA35c",
    "outputId": "4f5d8aa3-6447-4068-e83f-21414943dec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7828683483376415\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.87      9522\n",
      "           1       0.26      0.23      0.24      1697\n",
      "\n",
      "    accuracy                           0.78     11219\n",
      "   macro avg       0.56      0.56      0.56     11219\n",
      "weighted avg       0.77      0.78      0.78     11219\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[8394 1128]\n",
      " [1308  389]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ad9a0dc3510>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = best_svc.predict(X_test_scaled)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hbG2zMVA35c"
   },
   "source": [
    "### CNN results\n",
    "\n",
    "The initial model had a learning rate of 0.001, using the Adam optimizer, and a batch size of 32\n",
    "\n",
    "This model didn’t perform that poorly, but a question of which hyperparameters were best was yet to be answered. Because of this the optimization process would then turn into a gridsearch process where multiple arrays of hyperparameters would be used to run multiple different models, because of the large amount of Neural Networks this would create, some things had to be toned down, like number of epochs for example. Along with this in order to reduce runtime, images were scaled down to 16x16 pixels. This may have led to some incorrect classifications but also resulted in quicker results. Unfortunately, the typical parameters used to assess neural networks, that being f1-score, precision, and recall were not programmed in before running the model, so going based off accuracy of classification is the only parameter being discussed.\n",
    "\n",
    "\n",
    "The initial model is on the left, which had an okay accuracy of 86%, with 10 epochs to run through. The optimized model(s) however have varying results, The model with the configuration Adam, learning rate 0.01, and batch size 64 had the greatest overall loss, whereas the SGD, 0.01, 64, configuration maintained consistency, but was slower than the Adam variation. A model that does not do very well is the model with optimizer SGD, learning rate 0.01, and batch size 16, as the loss drop is not a significant, which indicates that the model is not learning as well, this may be due to the small batch sizes not doing well with the faster learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlXb_iBuA35c"
   },
   "outputs": [],
   "source": [
    "results_16 = {\n",
    "    (\"SGD\", 0.01, 16): [1445.4743, 1441.6831, 1438.9502, 1437.0728, 1433.2455],\n",
    "    (\"Adam\", 0.01, 16): [1475.2849, 1460.9638, 1460.2559, 1459.9407, 1460.0902],\n",
    "    (\"SGD\", 0.001, 16): [1492.5712, 1444.7881, 1443.2383, 1442.3714, 1441.5324],\n",
    "    (\"Adam\", 0.001, 16): [1389.5200, 1279.5868, 1181.0295, 1164.8745, 1148.8653]\n",
    "}\n",
    "\n",
    "results_32 = {\n",
    "    (\"SGD\", 0.01, 32): [730.2184, 720.4164, 719.2340, 719.3720, 718.7629],\n",
    "    (\"Adam\", 0.01, 32): [713.6464, 670.1269, 645.0783, 616.9844, 592.5035]\n",
    "}\n",
    "\n",
    "results_64 = {\n",
    "    (\"SGD\", 0.01, 64): [366.5066, 360.8374, 359.9431, 359.1196, 358.6196],\n",
    "    (\"Adam\", 0.01, 64): [364.7134, 356.2679, 341.4328, 324.2015, 315.0432],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (opt, lr, bs), losses in results_16.items():\n",
    "    label = f\"{opt}, lr={lr}, bs={bs}\"\n",
    "    plt.plot(range(1, len(losses)+1), losses, label=label)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch for Different Hyperparameters')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (opt, lr, bs), losses in results_32.items():\n",
    "    label = f\"{opt}, lr={lr}, bs={bs}\"\n",
    "    plt.plot(range(1, len(losses)+1), losses, label=label)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Epoch for different Hyperparameters\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (opt, lr, bs), losses in results_64.items():\n",
    "    label = f\"{opt}, lr={lr}, bs={bs}\"\n",
    "    plt.plot(range(1, len(losses)+1), losses, label=label)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Epoch for different Hyperparameters\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfqqnObIA35c"
   },
   "source": [
    "## Discussion and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion on the SVC Results\n",
    "\n",
    "Although the best SVC model found by GridSearchCV is moderately good with an accuracy of around 78.3%, it still has a hard time trying to correctly classify images of concrete that have cracks as evidenced by the differences between correctly identified classes (88.2% of non-cracked identified correctly vs. 22.9% of cracked identified correctly). These results could be improved slightly in the future by adding more C values and gamma values to GridSearchCV, as it is possible that a better SVC model could exist for the data.\n",
    "\n",
    "The SVC had two main challenges when attempting to classify the data. The first was that there was many more images of non-cracked concrete than there was images of cracked concrete. This could be fixed in the future by using a different method to balance the classes more equally. The second challenge was the small and complex patterns within the images causing the divide between classes. This issue is hard to fix using an SVC as these patterns are hard for the SVC's kernel functions to capture effectively. The only way to get around this issue would be by using some other model to classify the data, such as a CNN or a more advanced deep learning architecture like a ResNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsEYB7isA35h"
   },
   "source": [
    "### Discussion on the CNN Results\n",
    "The optimized model unfortunately had to be cut short, as 48 hours in, we had around 9 CNNs that had run with a selection of different hyperparameters. Some models had a steadily decreasing loss which indicates that the models were learning at an okay rate. Due to the model not being able to test all parameters, we don’t have a full set of results to analyze, and we don’t have all of our assessment parameters to work with either\n",
    "\n",
    "In the future, this optimization process will have all assessment feature (precision, recall, f1-score and accuracy) implemented in order to analyze the model completely In addition to this, the model would ideally be run on the images in a larger, more interpretable resolution. Not to mention, the selection of hyperparameters will all be evaluated, even if it takes multiple days to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FV7AxGoA35h"
   },
   "source": [
    "## Author contribution\n",
    "\n",
    "Colby Degan (degancol@msu.edu) - Responsible for making the SVC\n",
    "\n",
    "Jose Villegas (Villeg45@msu.edu) - Responsible for making the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzWeu0nMA35h"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsoyKxQlA35h"
   },
   "source": [
    "Geeksforgeeks. (2025). Building a Convolutional Neural Network using PyTorch. https://www.geeksforgeeks.org/building-a-convolutional-neural-network-using-pytorch/\n",
    "\n",
    "Imbalanced Learn. (n.d.). SMOTE — Version 0.9.0. Imbalanced-Learn.org. https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\n",
    "\n",
    "Pytorch. PyTorch documentation.  https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "SciKit Learn. (2019). sklearn.model_selection.GridSearchCV — scikit-learn 0.22 Documentation. Scikit-Learn.org. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
