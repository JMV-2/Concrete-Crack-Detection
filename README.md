# Concrete-Crack-Detection
This repository contains the Concrete Crack Detection code, By Colby Degan and Jose Villegas

## Background and Motivation
When it comes to classification problems in Data Science and Machine Learning, the utility of images can often be very useful to do so. For this particular project, a large directory of images was provided, within the directory there were two sub-directories split into categories of Cracked, and Non-cracked, then within those directories there were nested directories that consisted of the type, or structure of concrete, which were of the following: pavement, deck, and wall. Our overall goal was to consider two machine learning processes/algorithms that we learned in class, and apply those models to these different kinds of images to make a model that can optimally classify whether or not images are cracked. After these models are created, these models will be optimized utilizing some form of cross validation to make improvements.

## Methodology
There were two approaches that we utilized to create our models, the first of which was an Support Vector Classifier that was run on each of the three different structures, then the optimization approach was done using the GridSearchCV class from Sklearn. the other approach was done thorough A Convolutional neural network. Which is a neural network that excels in image classification, and attempts to mimic similar classification approaches that humans make. The optimization to the CNN will be to utilize gridsearch or apply dropout regularization to optimize the CNN.

## Data
We are compressing each 256x256 image into 16x16, and are using each pixel measurement of that compressed image as a variable, giving us 256 pixel measurements. We are also using the type of structure as a variable, as it wouldn't be logical to compare a picture of cracked pavement to a picture of an un-cracked wall.

## Results
The most optimal SVC found by GridSearchCV obtain an accuracy of around 78.3% when using the scaled, balanced data. This isn't necessarily a bad accuracy, but we would prefer to have one higher, preferably around 90%. The precision, recall, and f1-score for no crack being found were 0.87, 0.88, and 0.87, respectively, while those for a crack being found were 0.26, 0.23, and 0.24. This is likely due to there being more images of concrete without cracks than those with cracks, as evidenced by the support values of 9522 for no cracks found and 1697 for a crack being found.

We additionally generated a confusion matrix to better understand and visualize the model's performance across the classes. The confusion matrix showcases that there were 1128 false positives and 1308 false negatives. It also shows that around 88.2% of the images without cracks were identified correctly, while only 22.9% of the images with cracks were identified correctly. As mentioned earlier this is likely due to the difference in size between the two classes. Another likely cause of this are the small, complex patterns that cracks cause in the image data, which SVCs have a hard time noticing.

The initial model had a learning rate of 0.001, using the Adam optimizer, and a batch size of 32

This model didn’t perform that poorly, but a question of which hyperparameters were best was yet to be answered. Because of this the optimization process would then turn into a gridsearch process where multiple arrays of hyperparameters would be used to run multiple different models, because of the large amount of Neural Networks this would create, some things had to be toned down, like number of epochs for example. Along with this in order to reduce runtime, images were scaled down to 16x16 pixels. This may have led to some incorrect classifications but also resulted in quicker results. Unfortunately, the typical parameters used to assess neural networks, that being f1-score, precision, and recall were not programmed in before running the model, so going based off accuracy of classification is the only parameter being discussed.

The initial model is on the left, which had an okay accuracy of 86%, with 10 epochs to run through. The optimized model(s) however have varying results, The model with the configuration Adam, learning rate 0.01, and batch size 64 had the greatest overall loss, whereas the SGD, 0.01, 64, configuration maintained consistency, but was slower than the Adam variation. A model that does not do very well is the model with optimizer SGD, learning rate 0.01, and batch size 16, as the loss drop is not a significant, which indicates that the model is not learning as well, this may be due to the small batch sizes not doing well with the faster learning rate.

## Discussion on the SVC Results
Although the best SVC model found by GridSearchCV is moderately good with an accuracy of around 78.3%, it still has a hard time trying to correctly classify images of concrete that have cracks as evidenced by the differences between correctly identified classes (88.2% of non-cracked identified correctly vs. 22.9% of cracked identified correctly). These results could be improved slightly in the future by adding more C values and gamma values to GridSearchCV, as it is possible that a better SVC model could exist for the data.

The SVC had two main challenges when attempting to classify the data. The first was that there was many more images of non-cracked concrete than there was images of cracked concrete. This could be fixed in the future by using a different method to balance the classes more equally. The second challenge was the small and complex patterns within the images causing the divide between classes. This issue is hard to fix using an SVC as these patterns are hard for the SVC's kernel functions to capture effectively. The only way to get around this issue would be by using some other model to classify the data, such as a CNN or a more advanced deep learning architecture like a ResNet.

## Discussion on the CNN Results
The optimized model unfortunately had to be cut short, as 48 hours in, we had around 9 CNNs that had run with a selection of different hyperparameters. Some models had a steadily decreasing loss which indicates that the models were learning at an okay rate. Due to the model not being able to test all parameters, we don’t have a full set of results to analyze, and we don’t have all of our assessment parameters to work with either

In the future, this optimization process will have all assessment feature (precision, recall, f1-score and accuracy) implemented in order to analyze the model completely In addition to this, the model would ideally be run on the images in a larger, more interpretable resolution. Not to mention, the selection of hyperparameters will all be evaluated, even if it takes multiple days to run.
